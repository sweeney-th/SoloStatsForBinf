---
title: "Generative Models for Discrete Data.Rmd"
output: html_document
---

    https://web.stanford.edu/class/bios221/book/Chap-Generative.html

```{R}

set.seed(235569515)
```


## Real Example - HIV

    Let’s dive into a real example, where we know the probability model for the process. We are told that mutations along the genome of HIV (Human Immunodeficiency Virus) occur at random with a rate of 5×10−4 per nucleotide per replication cycle. This means that after one cycle, the number of mutations in a genome of about 10^4 = 10,000 nucleotides will follow a Poisson distribution (We will give more details later about this type of probability distribution. with rate 5). What does that tell us? This probability model predicts that the number of mutations over one replication cycle will be close to 5, and that the variability of this estimate is √5 (the standard error). We now have baseline reference values for both the number of mutations we expect to see in a typical HIV strain and its variability.

    In fact, we can deduce even more detailed information. If we want to know how often 3 mutations could occur under the Poisson(5) model, we can use an R function to generate the probability of seeing x=3 events, taking the value of the rate parameter of the Poisson distribution, called lambda, to be 5.

```{R Find Probability of Specific Value}

# odds of excactly 3 instances in a piosson distribution with rate 5
dpois(x = 3, lambda = 5)
```

```{R Finds Probabilities in a Range}

# pass a range using :
barplot(dpois(0:12, 5), names.arg = 0:12, col = "red")
```


The Poisson distribution is a good model for rare events such as mutations.

## Using discrete probability models

    A point mutation can either occur or not; it is a binary event. The two possible outcomes (yes, no) are called the levels of the categorical variable.
    
    Not all events are binary. For example, the genotypes in a diploid organism can take three levels (AA, Aa, aa).
    
    Sometimes the number of levels in a categorical variable is very large; examples include the number of different types of bacteria in a biological sample (hundreds or thousands) and the number of codons formed of 3 nucleotides (64 levels).
    
    When we measure a categorical variable on a sample, we often want to tally the frequencies of the different levels in a vector of counts. R has a special encoding for categorical variables and calls them factors. Here we capture the different blood genotypes for 19 subjects in a vector which we tabulate.
    
```{R Blood Types}

# Blood types of 19 hypoethetical people
genotype = c("AA","AO","BB","AO","OO","AO","AA","BO","BO",
             "AO","BB","AO","BO","AB","OO","AB","BB","AO","AO")
table(genotype)
```

    Tossing a coin has two possible outcomes. This simple experiment, called a Bernoulli trial, is modeled using a so-called Bernoulli random variable. Understanding this building block will take you surprisingly far. We can use it to build more complex models.
    
    Let’s try a few experiments to see what some of these random variables look like. We use special R functions tailored to generate outcomes for each type of distribution. They all start with the letter r, followed by a specification of the model, here rbinom, where binom is the abbreviation used for binomial.
    
    Suppose we want to simulate a sequence of 15 fair coin tosses. To get the outcome of 15 Bernoulli trials with a probability of success equal to 0.5 (a fair coin), we write
    
```{R Bernouli}

# fair coin 
rbinom(15, prob = 0.5, size = 1)
```

(Say a box has two sections, and the right is 2/3 of the box, the left, 1/3)


```{R Emulate the Box}

# emulate the 2/3 falling into the right of the box
rbinom(15, prob = 2/3, size = 1)

# to just get the number, not the vector of results:
rbinom(1, prob = 2/3, size = 12)

# probability mass distribution 
probabilities = dbinom(0:15, prob = 0.3, size = 15)
round(probabilities, 2)

# we can plot it thusly
barplot(probabilities, names.arg = 0:15, col = "blue")
```

## Poisson distributions

    When the probability of success p is small and the number of trials n large, the binomial distribution B(n,p) can be faithfully approximated by a simpler distribution, the Poisson distribution with rate parameter lambda = np. We already used this fact, and this distribution, in the HIV example.
    
### Question 1.5

        What is the probability mass distribution of observing 0:12 mutations in a genome of n=104 nucleotides, when the probability is p=5×10−4 per nuceotide? Is it similar when modeled by the binomial B(n,p) distribution and by the Poisson(λ=np) distribution?

---
    Note that, unlike the binomial distribution, the Poisson no longer depends on two separate parameters n and p, but only on their product np. As in the case of the binomial distribution, we also have a mathematical formula for computing Poisson probabilities.
    
    P(X=k) = lambda^k * e ^lambda/k!

```{R Code for Piosson}

# example - rate of 5, P(X=3) - will be like the first example
5^3 * exp(-5) / factorial(3)
```

    Simulate a mutation process along 10,000 positions with a mutation rate of 5×10−4 and count the number of mutations. Repeat this many times and plot the distribution with the barplot function

```{R Simulate Many Runs of Specified Example}

rbinom(1, prob = 5e-4, size = 10000)

simulations = rbinom(n = 300000, prob = 5e-4, size = 10000)
barplot(table(simulations), col = "lavender")
```


## Generateive Model for Epitope Prediction

Let's say we're a hypothetical elisa-based experiment.

    The baseline noise level per position, or more precisely the false positive rate, is 1%. This is the probability of declaring a hit – we think we have an epitope – when there is none. We write this P( declare epitope | no epitope)
    
    The protein is tested at 100 different positions, supposed to be independent.
    
    We are going to examine a collection of 50 patient samples.
    
Each patients' results are presented as a vector of length 100 with a 0 being not hit and a 1 being a hit (and a potential for an allergic reaction).

Our goal is:

     Verify by simulation that the sum of 50 independent Bernoulli variables with p = 0.01 is –to good enough approximation– the same as a Poisson(0.5) random variable.

We know that, due the false positive rate, there is a 1% chance a result is due to chance.

```{R Get Data}

# load it randomly into the namespace because we hate ourselves
load("data/e100.RData")

# plot the results of our simulated experiment
barplot(e100, ylim = c(0, 7), width = 0.7, xlim = c(-0.5, 100.5),
  names.arg = seq(along = e100), col = "darkolivegreen")
```

Obviously, we see the 7 right away. We'd like to ask ourselves what the odds are that we get such a result. It's tempting to ask what are the odds, given Piosson(0.5), that we get a 7, though the more appropriate question is what are the odds, given 100 repititions, that 7 is the maximum.

The walkthrough of this is in my notebook. I will have to scan it. 

The takeaway here is that we can calculate it in R explicity but are probably better off running a monte carlo simulation:

```{R Monte Carlo Simulations for ELISA}

set.seed(123)

# run 100,000 simulations of a random piosson distribution of 100 values with l = 0.5
maxes = replicate(100000, {
  # find the max each time
  max(rpois(100, 0.5))
})

# show a table of the max values
table(maxes)


```

We see that the max was 7 or higher 10 times.

```{R Find Odds}

# we can find the answer thusly
sum(maxes >=7)/length(maxes)

# or we can use this shortcut getting 1s and 0s from the boolean vector
# find the mean of the boolean vector'
mean(maxes >= 7)
```


## Multinomial Dustributions

With DNA, there are four normal nucleotides, ATCG. The odds still add up to the total, ie, pA+pT+pG+pC = 1. 

### Q 1.7 

    Suppose we have four boxes that are equally likely. Using the formula, what is the probability of observing 4 in the first box, 2 in the second box, and none in the two other boxes?
    


```{R Uniform Multinomial of 4}

# 4 values with odds of 1/4 each
pvec = rep(1/4, 4) #  0.25 0.25 0.25 0.25

t(rmultinom(1, prob = pvec, size = 8))
```


### Question 1.9

How do you interpret the difference between rmultinom(n = 8, prob = pvec, size = 1) and rmultinom(n = 1, prob = pvec, size = 8)? Hint: remember what we did in Sections 1.3.1 and 1.3.2.

```{R Difference in n}

# this has 1 trials of 1/4 8 times
rmultinom(n = 8, prob = pvec, size = 1)

# this has 8 trials of 1/4 once
rmultinom(n = 1, prob = pvec, size = 8)
```

## 1.4.1 Simulating for Power

    Let’s see an example of using Monte Carlo for the multinomial in a way which is related to a problem scientists often have to solve when planning their experiments: how big a sample size do I need?
    
    The term power has a special meaning in statistics. It is the probability of detecting something if it is there, also called the true positive rate.
    
    Conventionally, experimentalists aim for a power of 80% (or more) when planning experiments. This means that if the same experiment is run many times, about 20% of the time it will fail to yield significant results even though it should.
    
    Let’s call H0 the null hypothesis that the DNA data we have collected comes from a fair process, where each of the 4 nucleotides is equally likely
    
    Let’s call H0 the null hypothesis that the DNA data we have collected comes from a fair process, where each of the 4 nucleotides is equally likely (pA,pC,pG,pT)=(0.25,0.25,0.25,0.25). Null here just means: the baseline, where nothing interesting is going on. It’s the strawman that we are trying to disprove (or “reject”, in the lingo of statisticians), so the null hypothesis should be such that deviations from it are interesting.
    
    As you saw by running the R commands for 8 characters and 4 equally likely outcomes, represented by equal-sized boxes, we do not always get 2 in each box. It is impossible to say, from looking at just 8 characters, whether the nucleotides come from a fair process or not.
    
    Let’s determine if, by looking at a sequence of length n=20, we can detect whether the original distribution of nucleotides is fair or whether it comes from some other (“alternative”) process.
    
    We generate 1000 simulations from the null hypothesis using the rmultinom function. We display only the first 11 columns to save space.
    
```{R 1000 Null Hypothesis}


obsunder0 = rmultinom(1000, prob = pvec, size = 20)

dim(obsunder0)

obsunder0[, 1:11]
```

### Creating a test (normal variation)

    Remember: we know these values come from a fair process. Clearly, knowing the process’ expected values isn’t enough. We also need a measure of variability that will enable us to describe how much variability is expected and how much is too much. We use as our measure the following statistic, which is computed as the sum of the squares of the differences between the observed values and the expected values relative to the expected values. Thus, for each instance, 

\begin{equation}
{\tt stat}=\frac{(E_A-x_A)^2}{E_A}+
\frac{(E_C-x_C)^2}{E_C}+
\frac{(E_G-x_G)^2}{E_G}+
\frac{(E_T-x_T)^2}{E_T}
=\sum_i\frac{(E_i-x_i)^2}{E_i}
\tag{1.1}
\end{equation}

    How much do the first three columns of the generated data differ from what we expect? 

```{R Variablity in the columns}

# 20 observations
expected0 = pvec * 20

# explicitly see the first 3 columns
sum((obsunder0[, 1] - expected0)^2 / expected0)

sum((obsunder0[, 2] - expected0)^2 / expected0)

sum((obsunder0[, 3] - expected0)^2 / expected0)

# a function to calculate on all of them
stat = function(obsvd, exptd = 20 * pvec) {
  sum((obsvd - exptd)^2 / exptd)
}
stat(obsunder0[, 1])
```

    To get a more complete picture of this variation, we compute the measure for all 1000 instances and store these values in a vector we call S0: it contains values generated under H0. We can consider the histogram of the S0 values shown in Figure 1.10 an estimate of our null distribution.
    
```{R Visualize}

S0 = apply(obsunder0, 2, stat)

head(S0)

summary(S0)

hist(S0, breaks = 25, col = "lavender", main = "")
```

Apply quantiles to the data

```{R Quantiles}

q95 = quantile(S0, probs = 0.95)
q95
```

    So we see that 5% of the S0 values are larger than 7.6. We’ll propose this as our critical value for testing data and will reject the hypothesis that the data come from a fair process, with equally likely nucleotides, if the weighted sum of squares stat is larger than 7.6.
    
#### Determining our test’s power

    We must compute the probability that our test –based on the weighted sum-of-square differences– will detect that the data in fact do not come from the null hypothesis. We compute the probability of rejecting by simulation. We generate 1000 simulated instances from an alternative process, parameterized by pvecA.
    
```{R Alternative Process to Compare Our Test To}

pvecA = c(3/8, 1/4, 3/12, 1/8) # randomly chosen I think?

# run the test again
observed = rmultinom(1000, prob = pvecA, size = 20)

# apply our variance function
S1 = apply(observed, 2, stat)

# which are higher than the 95th (of the previous test???)
sum(S1 > q95)

power = mean(S1 > q95)
power
```


